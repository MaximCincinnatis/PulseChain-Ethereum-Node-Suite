version: '3.8'

# Common configuration that can be reused
x-common-variables: &common-variables
  # Network selection (either "pulsechain" or "ethereum")
  NETWORK: ${NETWORK:-pulsechain}
  # Node type (full or archive)
  NODE_TYPE: ${NODE_TYPE:-archive}
  # Client selection
  EXECUTION_CLIENT: ${EXECUTION_CLIENT:-geth}
  CONSENSUS_CLIENT: ${CONSENSUS_CLIENT:-lighthouse}
  # Operation mode
  NODE_OPERATION_MODE: ${NODE_OPERATION_MODE:-LOCAL_INTENSIVE}

# Common logging configuration
x-logging: &logging-config
  driver: "json-file"
  options:
    max-size: "100m"
    max-file: "5"
    compress: "true"
    labels: "service-logs"

# Common healthcheck configuration
x-healthcheck: &healthcheck-config
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 60s

services:
  # Execution client (Geth/Erigon)
  execution:
    image: ${EXECUTION_CLIENT_IMAGE:-ethereum/client-go:latest}
    container_name: execution
    restart: unless-stopped
    volumes:
      - type: volume
        source: chaindata
        target: /chaindata
        volume:
          nocopy: true
      - type: bind
        source: ./config
        target: /config
        read_only: true
      - type: bind
        source: ${CUSTOM_PATH}/.secrets
        target: /secrets
        read_only: true
    ports:
      - "${API_ADDR}:8545:8545"  # RPC
      - "${API_ADDR}:8546:8546"  # WebSocket
      - "30303:30303/tcp"  # P2P TCP
      - "30303:30303/udp"  # P2P UDP
    deploy:
      resources:
        limits:
          cpus: '${EXECUTION_CPU_LIMIT}'
          memory: ${EXECUTION_MEMORY_LIMIT}G
        reservations:
          cpus: '2'
          memory: 8G
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    environment:
      <<: *common-variables
      EXECUTION_CACHE_SIZE: ${EXECUTION_CACHE_SIZE}
      JWT_SECRET_FILE: /secrets/jwt.hex
      INDEXING_MODE: ${INDEXING_MODE}
      INDEX_BATCH_SIZE: ${INDEX_BATCH_SIZE}
      DB_WRITE_BUFFER_SIZE: ${DB_WRITE_BUFFER_SIZE}
      DB_BLOCK_CACHE_SIZE: ${DB_BLOCK_CACHE_SIZE}
      DB_MAX_OPEN_FILES: ${DB_MAX_OPEN_FILES}
    command: >
      --datadir=/chaindata
      --config=/config/${NETWORK}_${EXECUTION_CLIENT}.toml
      --http
      --http.addr=0.0.0.0
      --http.port=8545
      --http.api=${RPC_ALLOWED_METHODS}
      --http.corsdomain=${API_CORS_DOMAINS}
      --http.vhosts=${API_VHOSTS}
      --ws
      --ws.addr=0.0.0.0
      --ws.port=8546
      --ws.api=${RPC_ALLOWED_METHODS}
      --ws.origins=${API_CORS_DOMAINS}
      --maxpeers=${EXECUTION_MAX_PEERS}
      --cache=${EXECUTION_CACHE_SIZE}
      --cache.database=${DB_BLOCK_CACHE_SIZE}
      --cache.trie=${INDEX_CACHE_SIZE_GB}G
      --txlookuplimit=0
      --state.scheme=path
      --syncmode=full
      --db.engine=pebble
      --metrics
      --metrics.addr=0.0.0.0
      --metrics.port=6060
      --pprof
      --pprof.addr=0.0.0.0
      --pprof.port=6061
      --gcmode=archive
      --txpool.globalslots=${MEMPOOL_MAX_SLOTS}
      --txpool.globalqueue=${MEMPOOL_MAX_SLOTS}
      --rpc.allow-unprotected-txs=true
      --rpc.batch-request-limit=100
      --rpc.batch-response-max-size=100
      --rpc.enabledeprecatedpersonal
      --snapshot=false
    logging: *logging-config
    healthcheck:
      <<: *healthcheck-config
      test: ["CMD", "curl", "-f", "http://localhost:8545"]

  # Consensus client (Lighthouse/Prysm)
  consensus:
    image: ${CONSENSUS_CLIENT_IMAGE:-sigp/lighthouse:latest}
    container_name: consensus
    restart: unless-stopped
    volumes:
      - type: volume
        source: beacondata
        target: /beacondata
        volume:
          nocopy: true
      - type: bind
        source: ./config
        target: /config
        read_only: true
      - type: bind
        source: ${CUSTOM_PATH}/.secrets
        target: /secrets
        read_only: true
    ports:
      - "9000:9000/tcp"  # P2P TCP
      - "9000:9000/udp"  # P2P UDP
      - "127.0.0.1:5052:5052"  # HTTP API - localhost only
    deploy:
      resources:
        limits:
          cpus: '${CONSENSUS_CPU_LIMIT}'
          memory: ${CONSENSUS_MEMORY_LIMIT}G
        reservations:
          cpus: '1'
          memory: 4G
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    environment:
      <<: *common-variables
      JWT_SECRET_FILE: /secrets/jwt.hex
    command: >
      beacon_node
      --datadir=/beacondata
      --execution-endpoint=http://execution:8551
      --execution-jwt=/secrets/jwt.hex
      --http
      --http-address=0.0.0.0
      --metrics
      --metrics-address=0.0.0.0
      --network=${NETWORK}
    depends_on:
      execution:
        condition: service_healthy
    logging: *logging-config
    healthcheck:
      <<: *healthcheck-config
      test: ["CMD", "curl", "-f", "http://localhost:5052/eth/v1/node/health"]

  # Prometheus monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    user: nobody:nogroup
    volumes:
      - type: bind
        source: ./monitoring/prometheus
        target: /etc/prometheus
        read_only: true
      - type: volume
        source: prometheus_data
        target: /prometheus
    ports:
      - "127.0.0.1:9090:9090"  # localhost only
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.time=${METRICS_RETENTION_DAYS}d
      - --web.console.libraries=/usr/share/prometheus/console_libraries
      - --web.console.templates=/usr/share/prometheus/consoles
    logging: *logging-config
    healthcheck:
      <<: *healthcheck-config
      test: ["CMD", "wget", "-q", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]

  # Grafana dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    user: grafana
    volumes:
      - type: volume
        source: grafana_data
        target: /var/lib/grafana
      - type: bind
        source: ./monitoring/grafana/provisioning
        target: /etc/grafana/provisioning
        read_only: true
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_SECURITY_ALLOW_EMBEDDING=false
      - GF_SECURITY_DISABLE_GRAVATAR=true
      - GF_SECURITY_COOKIE_SECURE=true
      - GF_SECURITY_STRICT_TRANSPORT_SECURITY=true
    ports:
      - "127.0.0.1:3000:3000"  # localhost only
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 512M
    depends_on:
      prometheus:
        condition: service_healthy
    logging: *logging-config
    healthcheck:
      <<: *healthcheck-config
      test: ["CMD", "wget", "-q", "--tries=1", "--spider", "http://localhost:3000/api/health"]

  # Add new mempool monitoring service
  mempool_monitor:
    image: alpine:latest
    container_name: mempool_monitor
    restart: unless-stopped
    volumes:
      - type: bind
        source: ./helper/mempool_monitor.sh
        target: /usr/local/bin/mempool_monitor.sh
        read_only: true
      - type: bind
        source: ${LOG_PATH}
        target: /var/log/mempool
    environment:
      <<: *common-variables
      MEMPOOL_MONITORING: ${MEMPOOL_MONITORING}
      MEMPOOL_METRICS_INTERVAL: ${MEMPOOL_METRICS_INTERVAL}
      MEMPOOL_MAX_SLOTS: ${MEMPOOL_MAX_SLOTS}
      MEMPOOL_HISTORY_HOURS: ${MEMPOOL_HISTORY_HOURS}
    command: sh -c "chmod +x /usr/local/bin/mempool_monitor.sh && /usr/local/bin/mempool_monitor.sh"
    depends_on:
      execution:
        condition: service_healthy

# Define networks with proper isolation
networks:
  execution_network:
    driver: bridge
    internal: true  # No external connectivity
  monitoring_network:
    driver: bridge
    internal: true  # No external connectivity

# Define named volumes with proper configuration
volumes:
  chaindata:
    driver: local
    driver_opts:
      type: none
      device: ${CUSTOM_PATH}/chaindata
      o: bind
  beacondata:
    driver: local
    driver_opts:
      type: none
      device: ${CUSTOM_PATH}/beacondata
      o: bind
  prometheus_data:
    driver: local
    driver_opts:
      type: none
      device: ${CUSTOM_PATH}/prometheus
      o: bind
  grafana_data:
    driver: local
    driver_opts:
      type: none
      device: ${CUSTOM_PATH}/grafana
      o: bind 